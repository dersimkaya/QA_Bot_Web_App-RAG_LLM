{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4a0395",
   "metadata": {},
   "source": [
    "# QA Bot Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bece4",
   "metadata": {},
   "source": [
    "In this project, we build a complete Question‑Answering (QA) system that can respond to user queries using the content of uploaded documents. \n",
    "The goal is to construct a QA bot that leverages LangChain and a Large Language Model (LLM) to answer questions directly from the documents we load into the system.\n",
    "\n",
    "Instead of relying on predefined rules or static responses, the bot uses a modern technique called Retrieval‑Augmented Generation (RAG). This approach allows the model to combine two powerful capabilities:\n",
    "\n",
    "- **Retrieval:** The bot searches through the uploaded documents to find the most relevant pieces of information.\n",
    "\n",
    "- **Generation:** The bot passes the retrieved context to a Large Language Model (LLM), which then generates a natural‑language answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bfb4c",
   "metadata": {},
   "source": [
    "By the end, this project becomes a fully functional RAG pipeline that transforms raw documents into an interactive, document‑aware assistant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88825223",
   "metadata": {},
   "source": [
    "## Importing The Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57556052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this section to suppress warnings generated by your code:\n",
    "import warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b64ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8426bb7",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae84e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Document loader for multiple files\n",
    "####\n",
    "\n",
    "def document_loader(file_paths: List[str]):\n",
    "    \"\"\"\n",
    "    file_paths: list of local file paths (strings) from Gradio when type='filepath' and file_count='multiple'\n",
    "    returns: list of langchain.Document objects with metadata['source'] set to filename\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "        # Add source metadata so answers can reference file names/pages\n",
    "        for d in docs:\n",
    "            # PyPDFLoader usually sets metadata['source'] but ensure it's present and include filename\n",
    "            d.metadata = dict(d.metadata or {})\n",
    "            d.metadata[\"source_file\"] = path\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ad5bd",
   "metadata": {},
   "source": [
    "## Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ab5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Text splitter\n",
    "####\n",
    "\n",
    "def text_splitter(documents):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 100,\n",
    "        length_function = len,\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798c204",
   "metadata": {},
   "source": [
    "## Embeddings and Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "661452f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Embeddings and vector db\n",
    "####\n",
    "\n",
    "def bge_embedding():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name = \"BAAI/bge-base-en-v1.5\",\n",
    "        # Change device to \"cuda\" if you have GPU:\n",
    "        model_kwargs = {\"device\": \"cpu\"},\n",
    "        # To improve cosine similarity and retrieval quality:\n",
    "        encode_kwargs = {\"normalize_embeddings\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e2330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Vector DB\n",
    "####\n",
    "\n",
    "def vector_database(chunks):\n",
    "    embedding_model = bge_embedding()\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents = chunks,\n",
    "        embedding = embedding_model\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e1c82",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92cd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Retriever pipeline\n",
    "####\n",
    "\n",
    "def retriever_from_files(file_paths: List[str]):\n",
    "    docs = document_loader(file_paths)\n",
    "    chunks = text_splitter(docs)\n",
    "    vectordb = vector_database(chunks)\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56572179",
   "metadata": {},
   "source": [
    "## Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0389b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "# LLM\n",
    "####\n",
    "\n",
    "def get_llm():\n",
    "    \"\"\" \n",
    "    Initializes and returns a local Large Language Model (LLM) wrapped \n",
    "    as a LangChain-compatible pipeline. \n",
    "    \n",
    "    This function loads a pretrained language model and tokenizer \n",
    "    from Hugging Face (in this case, 'google/gemma-2b'), configures it \n",
    "    for text‑generation, and wraps the resulting pipeline so it can be used \n",
    "    inside LangChain chains such as RetrievalQA. \n",
    "    \"\"\"\n",
    "\n",
    "    # Model ID from Hugging Face\n",
    "    model_id = \"google/gemma-2b\"  \n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        # trust_remote_code= True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # trust_remote_code= True,\n",
    "        device_map = \"cpu\",    # \"auto\" if GPU is available\n",
    "        torch_dtype = None,\n",
    "    )\n",
    "\n",
    "    # Create a text-generation pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        # Control the maximum number of tokens in the generated output:\n",
    "        max_new_tokens = 256,\n",
    "        # Randomness or creativity of the model's responses:\n",
    "        temperature = 0.5,\n",
    "        # Pick next word based on probability + randomness\n",
    "        do_sample = True,\n",
    "        # return_full_text= False,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline= pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7a50b",
   "metadata": {},
   "source": [
    "## QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5416155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# QA chain\n",
    "####\n",
    "\n",
    "def retriever_qa(file_paths: List[str], query: str):\n",
    "    # file_paths is a list of strings when Gradio passes multiple files\n",
    "    if not file_paths:\n",
    "        return \"Please upload at least one PDF file.\"\n",
    "\n",
    "    llm= get_llm()\n",
    "    retriever_obj = retriever_from_files(file_paths)\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = retriever_obj,\n",
    "        return_source_documents = False\n",
    "    )\n",
    "\n",
    "    response = qa.invoke(query)\n",
    "    return response['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904ac58",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb929c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Gradio interface\n",
    "####\n",
    "\n",
    "rag_app = gr.Interface(\n",
    "    fn= retriever_qa,\n",
    "    flagging_mode = \"never\", \n",
    "    inputs = [\n",
    "        # Drag and drop file upload:\n",
    "        gr.File(\n",
    "            label = \"Upload PDF Files\",\n",
    "            file_count = \"multiple\",\n",
    "            file_types = [\".pdf\"],\n",
    "            type = \"filepath\"\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Input Query\",\n",
    "            lines = 3,\n",
    "            placeholder = \"Type your question here...\"\n",
    "        )\n",
    "    ],\n",
    "    outputs = gr.Textbox(\n",
    "        label = \"Output\",\n",
    "        lines = 5,\n",
    "    ),\n",
    "    title = \"RAG Chatbot\",\n",
    "    description = \"Upload PDF documents and ask a question. The chatbot will combine documents and answer from them.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb84843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rag_app.launch(\n",
    "        server_name = '0.0.0.0',\n",
    "        server_port = 7860,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e70f18",
   "metadata": {},
   "source": [
    "# PDF and environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebd5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export -n rag > environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476b80f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook NB.ipynb to webpdf\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 158624 bytes to NB.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to webpdf NB.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
